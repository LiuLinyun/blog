<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="学习笔记、读书笔记、生活感悟">
<meta property="og:type" content="website">
<meta property="og:title" content="LiuLin的学习笔记">
<meta property="og:url" content="https://liulinyun.github.io/index.html">
<meta property="og:site_name" content="LiuLin的学习笔记">
<meta property="og:description" content="学习笔记、读书笔记、生活感悟">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LiuLin的学习笔记">
<meta name="twitter:description" content="学习笔记、读书笔记、生活感悟">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://liulinyun.github.io/"/>





  <title>LiuLin的学习笔记</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LiuLin的学习笔记</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">前方道路畅通！！！</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://liulinyun.github.io/2018/03/29/支持向量机/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="流粼">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiuLin的学习笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/29/支持向量机/" itemprop="url">支持向量机</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-29T15:53:11+08:00">
                2018-03-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h2 id="第六章-支持向量机">第六章 支持向量机</h2>
<h3 id="间隔与支持向量">6.1 间隔与支持向量</h3>
<p>给定训练样本集 <span class="math inline">\(D=\lbrace (\boldsymbol x_1,y_1),(\boldsymbol x_2,y_2), \dots ,(\boldsymbol x_m,y_m) \rbrace, y_i\in\lbrace -1, +1 \rbrace\)</span> , 分类学习最基本的想法就是训练集 <span class="math inline">\(D\)</span> 在样本空间中找到一个划分超平面，将不同类别的样本分开。但能将训练样本分开的划分超平面可能有很多，如图6.1所示，我们应该努力去找到哪一个呢？</p>
<p><img src="/2018/03/29/支持向量机/支持向量机_图6_1.png" alt="图6.1 存在多个划分超平面将两类训练样本分开"> <strong>图6.1</strong> 存在多个划分超平面将两类训练样本分开</p>
<p>直观上看，应该去找两类训练样本“正中间”的划分超平面，即图6.1中红色实线的那个，因为该划分超平面对训练样本局部扰动的“容忍”性能最好。</p>
<p>在样本空间中，划分超平面可通过如下现行方程来描述： <span class="math display">\[ \boldsymbol{w^Tx}+b=0 \tag{6.1} \]</span> 其中 $ w=(w_1;w_2;;w_d) $ 为法向量，决定了超平面的方向；<span class="math inline">\(b\)</span> 为位移项，决定了超平面与原点之间的距离。显然，划分超平面可被法向量 <span class="math inline">\(\boldsymbol w\)</span> 和位移 <span class="math inline">\(b\)</span> 确定，下面我们将其记为 <span class="math inline">\((w,b)\)</span> 。样本空间中任意点 <span class="math inline">\(x\)</span> 到超平面 <span class="math inline">\((w,b)\)</span> 的距离可写为 <span class="math display">\[ r=\frac {|\boldsymbol{w^Tx} + b|} {\| \boldsymbol{w} \|}  \tag{6.2}\]</span></p>
<p>假设超平面 <span class="math inline">\((\boldsymbol w,b)\)</span> 能将样本正确分类，即对于 <span class="math inline">\((\boldsymbol x_i,y_i) \in D\)</span>，若 <span class="math inline">\(y_i=+1\)</span>，则有 <span class="math inline">\(\boldsymbol{w^Tx}_i + b &gt; 0\)</span>；若 <span class="math inline">\(y_i = -1\)</span>，则有 <span class="math inline">\(\boldsymbol{w^Tx}_i + b &lt; 0\)</span>。令 <span class="math display">\[ \begin{cases}
\boldsymbol{w^Tx}_i + b \geqslant +1, &amp; y_i=+1 \\
\boldsymbol{w^Tx}_i + b \leqslant -1, &amp; y_i=-1 \\
\end{cases} \tag{6.3} \]</span></p>
<p>如图6.2所示，距离超平面最近的这几个训练样本使上式的等号成立，它们被称为“支持向量”，两个异类支持向量到超平面的距离之和为 <span class="math display">\[ \gamma = \frac 2 {\| \boldsymbol w \|} \tag{6.4}\]</span> 它被称为“间隔”</p>
<p><embed src="支持向量机/支持向量机_图6_2.jepg"> <strong>图6.2</strong> 支持向量与间隔</p>
<p>欲找到具有“最大间隔”的划分超平面，也就是要找到参数 <span class="math inline">\(\boldsymbol w\)</span> 和 <span class="math inline">\(b\)</span>，使得 <span class="math inline">\(\gamma\)</span> 最大，即 <span class="math display">\[ \begin{aligned} 
\max_{\boldsymbol w,b} \quad  &amp; \frac 2 {\| \boldsymbol w \|} \\ 
s.t. \quad &amp; y_i(\boldsymbol {w^Tx}_i + b) \geqslant 1, \quad i=1,2,\dots,m.
\end{aligned} \tag{6.5} \]</span></p>
<p>显然，为了最大化间隔，只需要最大化 <span class="math inline">\({\| \boldsymbol w \|}^{-1}\)</span>，这等价于最小化 <span class="math inline">\({\| \boldsymbol w \|}^2\)</span>。于是，该优化问题可写为 <span class="math display">\[ \begin{aligned} 
\min_{\boldsymbol w,b} \quad  &amp; \frac 1 2 {\| \boldsymbol w \|}^2 \\ 
s.t. \quad &amp; y_i(\boldsymbol {w^Tx}_i + b) \geqslant 1, \quad i=1,2,\dots,m.
\end{aligned} \tag{6.6}\]</span> 这就是支持向量机的基本型。</p>
<h3 id="对偶问题">6.2 对偶问题</h3>
<p>我们希望求解上式来得到最大间隔划分超平面所对应的模型 <span class="math display">\[ f(x) = \boldsymbol{w^Tx} + b \tag{6.7}\]</span> 其中 <span class="math inline">\(\boldsymbol w\)</span> 和 <span class="math inline">\(b\)</span> 是模型参数。注意到式(6.6)本身是一个凸二次规划问题，能直接用现成的优化计算包求解，但我们有更高效的办法。</p>
<p>对式(6.6)使用拉格朗日乘子法可得到其“对偶问题”。具体来说，对其中的每条约束添加拉格朗日乘子 <span class="math inline">\(\alpha_i \geqslant 0\)</span>，则该问题的拉格朗日函数可写为 <span class="math display">\[ L(\boldsymbol w,b,\boldsymbol \alpha) = \frac 1 2 {\| \boldsymbol w \|}^2 + \sum_{i=1}^{m}{\alpha_i(1-y_i(\boldsymbol {w^Tx_i} + b))} \tag{6.8} \]</span> 其中 <span class="math inline">\(\boldsymbol \alpha = (\alpha_1;\alpha_2;\dots;\alpha_m)\)</span>。</p>
<p>具体的推导过程如下： * 将式(6.6)写成标准型 <span class="math display">\[ \begin{aligned} 
\min_{\boldsymbol w,b} \quad  &amp; \frac 1 2 {\| \boldsymbol w \|}^2 \\ 
s.t. \quad &amp; 1 - y_i(\boldsymbol {w^Tx}_i + b) \leqslant 0, \quad i=1,2,\dots,m.
\end{aligned}\]</span> * 为毎个不等式约束添加拉格朗日乘子 <span class="math inline">\(\alpha_i \geqslant 0\)</span>，再与目标函数相加，变成如下优化问题： <span class="math display">\[ \begin{aligned} 
\min_{\boldsymbol w, b, \boldsymbol \alpha} &amp; \quad \frac 1 2 {\| \boldsymbol w \|}^2 + \sum_{i=1}^{m}{\alpha_i(1-y_i(\boldsymbol {w^Tx_i} + b))} \\
s.t. &amp; \quad \alpha_i \geqslant 0, \quad i = 1,2,\dots,m
\end{aligned} \]</span> * 上式的目标函数即是拉格朗日函数，求解该目标函数的最小值，当 <span class="math inline">\(\partial L / \partial \boldsymbol w = 0\)</span>，<span class="math inline">\(\partial L / \partial b = 0\)</span>，<span class="math inline">\(\partial L / \partial \boldsymbol \alpha = 0\)</span> 时取得。由于这三个等式中有三个未知数，分别是 <span class="math inline">\(\boldsymbol w\)</span>、<span class="math inline">\(b\)</span>、<span class="math inline">\(\boldsymbol \alpha\)</span>，因此可以考虑用其中一个未知数表示另外两个，考虑对 <span class="math inline">\(\boldsymbol w\)</span> 和 <span class="math inline">\(b\)</span> 求导,进而可得到式(6.9)和式(6.10) <span class="math display">\[ \begin{cases}
\dfrac {\partial L}  {\partial \boldsymbol w} = \boldsymbol w - \displaystyle \sum_{i=1}^{m}{\alpha_i y_i \boldsymbol x_i} = 0 \\
\dfrac {\partial L} {\partial b} = \displaystyle \sum_{i=1}^{m}{\alpha_i y_i} = 0
\end{cases}\]</span></p>
<p>令 <span class="math inline">\(L(\boldsymbol w,b,\boldsymbol \alpha)\)</span> 对 <span class="math inline">\(\boldsymbol w\)</span> 和 <span class="math inline">\(b\)</span> 的偏导为零可得 <span class="math display">\[ \boldsymbol w = \sum_{i=1}^{m}{\alpha_i y_i \boldsymbol x_i} \tag{6.9}\]</span> <span class="math display">\[ 0 = \sum_{i=1}^{m}{\alpha_i y_i} \tag{6.10} \]</span> 将式(6.9)带入(6.8)，即可将 <span class="math inline">\(L(\boldsymbol w,b,\boldsymbol \alpha)\)</span> 中的 <span class="math inline">\(\boldsymbol w\)</span> 和 <span class="math inline">\(b\)</span> 消去，再考虑(6.10)的约束，就得到(6.6)的对偶问题 <span class="math display">\[ \begin{aligned} 
\max_{\boldsymbol \alpha} \quad &amp; \sum_{i=1}^m{\alpha_i} - \frac 1 2 \sum_{i=1}^m \sum_{j=1}^m {\alpha_i \alpha_j y_i y_j \boldsymbol x_i^T \boldsymbol x_j} \\
s.t. \quad &amp; \sum_{i=1}^m {\alpha_i y_i} = 0 \\
&amp; \alpha_i \geqslant 0, \quad i=1,2,\dots,m
\end{aligned} \tag{6.11}\]</span> 解出 <span class="math inline">\(\boldsymbol \alpha\)</span> 后，求出 <span class="math inline">\(\boldsymbol w\)</span> 与 <span class="math inline">\(b\)</span> 即可得到模型 <span class="math display">\[ \begin{aligned}
f(x) &amp;= \boldsymbol {w^Tx} + b \\
     &amp;= \sum_{i=1}^m {\alpha_i y_i \boldsymbol {x_i^T x} + b}
\end{aligned} \tag{6.12} \]</span></p>
<p>从对偶问题(6.11)解出的 <span class="math inline">\(\alpha_i\)</span> 是式(6.8)中的拉格朗日乘子，它恰好对应着训练样本 <span class="math inline">\((\boldsymbol x_i,y_i)\)</span>。注意到式(6.6)中有不等式约束，因此上述过程需要满足KKT条件，即要求 <span class="math display">\[\begin{cases}
\alpha_i \geqslant 0 \\
y_i f(\boldsymbol x_i) - 1 \geqslant 0 \\
\alpha_i(y_i f(\boldsymbol x_i)-1) = 0
\end{cases} \tag{6.13} \]</span> 于是，对训练样本 <span class="math inline">\((\boldsymbol x_i,y_i)\)</span>，总有 <span class="math inline">\(\alpha_i=0\)</span> 或 <span class="math inline">\(y_i f(\boldsymbol x_i) = 1\)</span>。若 <span class="math inline">\(\alpha_i = 0\)</span>，则该样本不会在式 (6.12) 的求和中出现，也就不会对 <span class="math inline">\(f(x)\)</span> 产生任何影响；若 <span class="math inline">\(\alpha_i &gt; 0\)</span>，则必有 <span class="math inline">\(y_i f(\boldsymbol x_i) = 1\)</span>，所对应的样本点位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需要保留，最终模型仅仅与支持向量有关。</p>
<p>那么，如何求解式(6.11)呢？不难发现这是一个二次规划问题，可用通用的二次规划求解算法来求解；然而，该问题的规模正比于训练样本数，在实际的训练中会造成极大的训练开销，为了避开这个障碍，人们提出了很多高效算法，SMO是其中一个著名的代表。</p>
<p>SMO的基本思路式先固定 <span class="math inline">\(\alpha_i\)</span> 之外的所有参数，然后求 <span class="math inline">\(\alpha_i\)</span> 上的极值。由于存在约束 <span class="math inline">\(\sum_{i=1}^m {\alpha_i y_i} = 0\)</span>，若固定 <span class="math inline">\(\alpha_i\)</span> 之外的其他变量，则 <span class="math inline">\(\alpha_i\)</span> 可由其它变量导出，于是，SMO每次选择两个变量 <span class="math inline">\(\alpha_i\)</span> 和 <span class="math inline">\(\alpha_j\)</span>，并固定其它参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛</p>
<ul>
<li>选取一对需要更新的变量 <span class="math inline">\(\alpha_i\)</span> 和 <span class="math inline">\(\alpha_j\)</span>；</li>
<li>固定 <span class="math inline">\(\alpha_i\)</span> 和 <span class="math inline">\(\alpha_j\)</span> 以外的参数，求解式(6.11)获得更新后的 <span class="math inline">\(\alpha_i\)</span> 和 <span class="math inline">\(\alpha_j\)</span>。</li>
</ul>
<p>选取 <span class="math inline">\(\alpha_i\)</span> 和 <span class="math inline">\(\alpha_j\)</span> 的启发式策略如下：</p>
<ul>
<li>(外循环)交替执行以下步骤选取 <span class="math inline">\(\alpha_i\)</span>: 1.选择违反KKT条件的 <span class="math inline">\(\alpha_i\)</span> 作为第一个变量；2.遍历非边界样本集 <span class="math inline">\((0 &lt; \alpha &lt; C)\)</span>，其中 <span class="math inline">\(C\)</span> 为惩罚系数(有些数据难以分类时松弛式(6.13)中KKT条件的约束 <span class="math inline">\(\alpha_i\)</span> 的约束，从 <span class="math inline">\(\alpha_i \geqslant 0\)</span> 变为 <span class="math inline">\(0 \leqslant \alpha_i \leqslant C\)</span>)，选取其中违反KKT的 <span class="math inline">\(\alpha_i\)</span> 作为第一个变量；</li>
<li>(内循环)第二个变量 <span class="math inline">\(\alpha_j\)</span> 的选择希望使其有较大的变化。首先定义 <span class="math inline">\(E_k = f(\boldsymbol x_k) - y_k\)</span> 表示第 <span class="math inline">\(k\)</span> 个样本的预测值与真实值之差。由于 <span class="math inline">\(\alpha_j\)</span> 的选择依赖于最大化 <span class="math inline">\(| E_i - E_j |\)</span>，含义在于使两个样本之间间隔最大，所以，当 <span class="math inline">\(\alpha_i\)</span> 为正时选取 <span class="math inline">\(E\)</span> 最小的 <span class="math inline">\(\alpha\)</span> 作为 <span class="math inline">\(\alpha_j\)</span>，当 <span class="math inline">\(\alpha_i\)</span> 为负时选取 <span class="math inline">\(E\)</span> 最大的 <span class="math inline">\(\alpha\)</span> 作为 <span class="math inline">\(\alpha_j\)</span>。</li>
</ul>
<p>实际中，按如下办法判断 <span class="math inline">\(\alpha\)</span> 是否违反KKT条件： <span class="math display">\[ \text{KKT条件} \begin{cases}
\quad \alpha_i = 0 &amp; \implies y_i f(\boldsymbol x_i) \geqslant 1 \\
\quad \alpha_i = C &amp; \implies y_i f(\boldsymbol x_i) \leqslant 1 \\
0 &lt; \alpha_i &lt; C &amp; \implies y_i f(\boldsymbol x_i) = 1
\end{cases} \]</span></p>
<p>SMO算法之所以高效，恰恰由于在固定其它参数后，仅仅优化两个参数的过程能够做到非常高效。具体来说，仅仅考虑 <span class="math inline">\(\alpha_i\)</span> 和 <span class="math inline">\(\alpha_j\)</span> 时，式(6.11)中的约束可重写为 <span class="math display">\[ \alpha_i y_i + \alpha_j y_j = c, \quad \alpha_i \geqslant 0, \quad \alpha_j \geqslant 0 \tag{6.14} \]</span> 其中 <span class="math display">\[ c = -\sum_{k \neq i,j} {\alpha_k y_k} \tag{6.15} \]</span> 是使得 <span class="math inline">\(\sum_{i=1}^m {\alpha_i y_i} = 0\)</span> 成立的常数。用 <span class="math display">\[ \alpha_i y_i + \alpha_j y_j = c \tag{6.16} \]</span> 消去式(6.11)中的变量 <span class="math inline">\(\alpha_j\)</span>，则得到一个关于 <span class="math inline">\(\alpha_i\)</span> 的单变量二次规划问题，仅有的约束是 <span class="math inline">\(\alpha_i \geqslant 0\)</span>。不难发现，这样的二次规划问题具有闭式解，于是不必调用数值优化算法即可高效算出更新后的 <span class="math inline">\(\alpha_i\)</span> 和 <span class="math inline">\(\alpha_j\)</span>。</p>
<p>如何确定偏移项 <span class="math inline">\(b\)</span> 呢？注意到对任意支持向量 <span class="math inline">\((\boldsymbol x_s, y_s)\)</span> 都有 <span class="math inline">\(y_s f(\boldsymbol x_s) = 1\)</span>，即 <span class="math display">\[ y_s \left ( \sum_{i \in S} {\alpha_i y_i \boldsymbol x_i^T \boldsymbol x_s} + b \right ) = 1 \tag{6.17} \]</span> 其中 <span class="math inline">\(S=\{i | \alpha_i &gt; 0, \quad i = 1,2,\dots,m \}\)</span> 为所有支持向量的下标集合。理论上，可以选用任意支持向量并通过求解式(t.17)获得 <span class="math inline">\(b\)</span>，但现实任务中常常采用更为鲁棒的做法：使用所有支持向量的平均值 <span class="math display">\[ b=\frac 1 {|S|} \sum_{s \in S} \left( \frac 1 y_s - \sum_{s \in S} {\alpha_i y_i \boldsymbol x_i^T \boldsymbol x_s} \right) \tag{6.18} \]</span></p>
<h2 id="与对率回归模型bp神经网络的联系">与对率回归模型，BP神经网络的联系</h2>
<p>如果软间隔SVM的损失函数使用対率损失函数，则得到的优化目标就相当于加上了正则化项的对率回归模型，也相当于只有一层全连接层和一层sigmoid函数输出的带正则化项的BP神经网络。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://liulinyun.github.io/2018/03/29/反向传播细节/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="流粼">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiuLin的学习笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/29/反向传播细节/" itemprop="url">反向传播细节</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-29T15:51:12+08:00">
                2018-03-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h1 id="反向传播细节">反向传播细节</h1>
<h2 id="反向传播和梯度下降概念辨析">反向传播和梯度下降概念辨析</h2>
<p>当我们使用前馈神经网络接收输入 <span class="math inline">\(\boldsymbol x\)</span> 并产生输出 <span class="math inline">\(\boldsymbol {\hat y}\)</span> 时，信息通过网络向前流动。输入 <span class="math inline">\(\boldsymbol x\)</span> 提供初始信息，然后传播到每一层的隐层单元，最终产生输出 <span class="math inline">\(\boldsymbol {\hat y}\)</span>。这称之为 <strong>前向传播</strong>。在训练过程中，前向传播可以持续向前知道它产生一个 <strong>标量代价函数</strong> <span class="math inline">\(J(\boldsymbol \theta)\)</span>。<strong>反向传播</strong> 算法，允许来自代价函数的信息通过网络向后流动，以便计算梯度。</p>
<p>反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上，<strong>反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯度下降，使用该梯度来进行学习</strong>。此外，反向传播经常被误解为仅适用于多层神经网络，但是 <strong>原则上反向传播可以计算任何函数的导数（可导）</strong> （对于一些函数，正确的响应是报告函数的导数是未定义的）。</p>
<h2 id="梯度下降随机梯度下降批量梯度下降">梯度下降，随机梯度下降，批量梯度下降</h2>
<p>梯度下降是指对整个数据集使用梯度下降算法更新参数； 随机梯度下降指对单个样本使用梯度下降算法更新参数； 批量梯度下降指对于部分样本使用梯度下降算法更新参数，是一种折中的办法。</p>
<h2 id="计算图">计算图</h2>
<p>以下是一些计算图的示例： <img src="/2018/03/29/反向传播细节/计算图.png" alt="使用 + 操作计算 z=x+y的图"></p>
<p>计算图中节点表示变量，有向边表示操作。</p>
<h2 id="微积分中的链式法则">微积分中的链式法则</h2>
<p>微积分中的链式法则用于计算复合函数的导数，<strong>反向传播是一种应用链式法则计算导数的算法</strong>，使用高效的特定运算顺序。 设 <span class="math inline">\(x\)</span> 是实数，<span class="math inline">\(f\)</span> 和 <span class="math inline">\(g\)</span> 是从实数映射到实数的函数。假设 <span class="math inline">\(y=g(x)\)</span> 并且 <span class="math inline">\(z=f(g(x)) = f(y)\)</span>。那么对应的链式法则为</p>
<p><span class="math display">\[ \frac {d z} {d x} = \frac {d z} {d y} \frac {d y} {d x} \]</span></p>
<p>我们可以将这种标量情况进行扩展。假设 <span class="math inline">\(\boldsymbol x \in \Bbb R^m, \boldsymbol y \in \Bbb R^n\)</span>，<span class="math inline">\(g\)</span> 是从 <span class="math inline">\(\Bbb R^m\)</span> 到 <span class="math inline">\(\Bbb R^n\)</span> 的映射，<span class="math inline">\(f\)</span> 是从 <span class="math inline">\(\Bbb R^n\)</span> 到 <span class="math inline">\(\Bbb R\)</span> 的映射。如果 <span class="math inline">\(\boldsymbol y=g(\boldsymbol x)\)</span> 并且 <span class="math inline">\(z=f(\boldsymbol y)\)</span>，那么</p>
<p><span class="math display">\[ \frac {\partial z} {\partial x_i}  = \sum_j { {\frac {\partial z}{\partial y_j} } {\frac {\partial y_j} {\partial x_i} } } \]</span></p>
<p>使用向量记法，可以等价地记为</p>
<p><span class="math display">\[ \triangledown_{\boldsymbol x}z = {\left(\frac {\partial \boldsymbol y}{\partial \boldsymbol x} \right)}^T  \triangledown_{\boldsymbol y}z \]</span></p>
<p>这里 <span class="math inline">\(\triangledown_{\boldsymbol x}z\)</span> 表示 <span class="math inline">\(z\)</span> 对 <span class="math inline">\(\boldsymbol x\)</span> 的梯度，<span class="math inline">\(\frac {\partial \boldsymbol y}{\partial \boldsymbol x}\)</span> 是 <span class="math inline">\(g\)</span> 的 <span class="math inline">\(n \times m\)</span> 的 Jacobian 矩阵。</p>
<p>通常，我们将反向传播算法应用于任意维度的张量，而不仅仅用于向量。从概念上讲，这与使用向量的反向传播完全相同。唯一的区别就是如何将数字排列成张量。我们可以想象，<strong>在运行反向传播之前，将毎个张量压扁变平为一个向量，计算一个向量值梯度，然后将该梯度重新构造成一个张量。</strong> 从这种重新排列的观点来看，反向传播仍然将 Jacobian 乘以梯度。</p>
<p>为了表示值 <span class="math inline">\(z\)</span> 关于张量 <span class="math inline">\(\mathsf X\)</span>，我们记为 <span class="math inline">\(\triangledown_{\mathsf X} z\)</span>，如果 <span class="math inline">\(\mathsf Y = g(\mathsf X)\)</span> 并且 <span class="math inline">\(z=f(\mathsf Y)\)</span>，那么</p>
<p><span class="math display">\[ \triangledown_{\mathsf X} z = \sum_j{ \left( \triangledown_{\mathsf X} \mathsf Y_j \right) \frac{\partial z}{\partial \mathsf Y_j}} \]</span></p>
<p>这里 <span class="math inline">\(\mathsf Y_j\)</span> 中的 <span class="math inline">\(j\)</span> 表示将 <span class="math inline">\(\mathsf Y\)</span> “压扁”后的索引。</p>
<h2 id="反向传播">反向传播</h2>
<p>举个栗子： 假如有如下神经网络结构 <img src="/2018/03/29/反向传播细节/示例神经网络.jpg" alt="示例神经网络"> 为了便于计算，损失函数 <span class="math inline">\(L\)</span> 可以写成：</p>
<p><span class="math display">\[ L(\hat y, y) = -y \log(\hat y) - (1-y)\log(1-\hat y) \]</span></p>
<p>它的输入特征有 3 个维度，假设共有 4 个数据，则</p>
<p><span class="math display">\[\begin{aligned}
\boldsymbol X &amp; = [\boldsymbol x^{(1)}, \boldsymbol x^{(2)}, \boldsymbol x^{(3)}, \boldsymbol x^{(4)}] \\
&amp; = \begin{bmatrix}
x_1^{(1)} &amp; x_1^{(2)} &amp; x_1^{(3)} &amp; x_1^{(4)} \\
x_2^{(1)} &amp; x_2^{(2)} &amp; x_2^{(3)} &amp; x_2^{(4)} \\
x_3^{(1)} &amp; x_3^{(2)} &amp; x_3^{(3)} &amp; x_3^{(4)}
\end{bmatrix}
\end{aligned}\]</span></p>
<p>依次从前往后计算，有</p>
<p><span class="math display">\[\begin{aligned}
\boldsymbol Z^{[1]} &amp; = {\boldsymbol W^{[1]}}^T \boldsymbol X + \boldsymbol b^{[1]} \\
\boldsymbol A^{[1]} &amp; = relu \left( \boldsymbol Z^{[1]} \right) \\
\boldsymbol Z^{[2]} &amp; = {\boldsymbol W^{[2]}}^T \boldsymbol A^{[1]} + \boldsymbol b^{[2]} \\
\boldsymbol {\hat y} &amp; = \sigma \left( \boldsymbol Z^{[2]} \right) = \frac 1 {1+e^{-\boldsymbol Z^{[2]}}} \\
\boldsymbol J &amp; = \frac 1 m \sum_i^m L({\hat y}_i, y_i)
\end{aligned}\]</span></p>
<p>最后计算出来的 <span class="math inline">\(\boldsymbol J\)</span> 是一个标量。整个计算过程所对应的计算图如下： <img src="/2018/03/29/反向传播细节/示例神经网络计算图.jpg" alt="示例神经网络计算图"> 得到损失 <span class="math inline">\(\boldsymbol J\)</span> 之后，对它进行反向求导得到参数的梯度，对于训练这个神经网络，我们需要得到 <span class="math inline">\(\boldsymbol J\)</span> 对于 <span class="math inline">\({\boldsymbol W^{[1]}}^T, \boldsymbol b^{[1]}, {\boldsymbol W^{[2]}}^T, \boldsymbol b^{[2]}\)</span> 的梯度，具体过程如下所示： <img src="/2018/03/29/反向传播细节/梯度求解示例.jpg" alt="梯度求解示例"> 从求解过程可以看出，最后的总体损失 <span class="math inline">\(\boldsymbol J\)</span> 对应某个矩阵或者张量参数的梯度，可以看成 <span class="math inline">\(\boldsymbol J\)</span> 对这个矩阵或者张量参数中的所有单一元素进行求导。具体深度学习框架的求解过程存在一些优化，可以参考花书进一步了解。 从上图求解过程的第二步求解 <span class="math inline">\(\triangledown_{\boldsymbol Z} {\hat y_i}\)</span> 可以看出，对于非全连接来说，只需要求解一部分相关变量 <span class="math inline">\(Z_i\)</span> 的导数，不需要求解其它无关变量的导数，减少了计算的时间复杂度和空间复杂度，这也就是参数共享加速学习的原理。</p>
<h2 id="反向传播之外的微分算法">反向传播之外的微分算法</h2>
<p>严格的按照由尾部递归向前的反向传播求解梯度的算法并不一定是时间复杂度最优的，确定计算顺序以达到运算开销最小是个NP难问题。严格的反向传播是<strong>反向模式累加</strong>的特殊形式，反向模式累加整体上是以反向计算求导，但对于某些步骤会先计算出中间的子表达式。</p>
<h2 id="附矩阵函数的-jacobian-矩阵">附：矩阵函数的 Jacobian 矩阵</h2>
<p>先通过列向量化，将 <span class="math inline">\(p \times q\)</span> 矩阵函数 <span class="math inline">\(\boldsymbol F(\boldsymbol X)\)</span> 转换成 <span class="math inline">\(pq \times 1\)</span> 列向量</p>
<p><span class="math display">\[ \mathrm {vec} (\boldsymbol F(\boldsymbol X)) \stackrel{\text{def}}{=} [f_{11}(\boldsymbol X), \dots, f_{p1}(\boldsymbol X), \dots, f_{1q}(\boldsymbol X), \dots, f_{pq}(\boldsymbol X)]^T \in \Bbb R^{pq} \]</span></p>
<p>然后，该列向量对矩阵函数变元 <span class="math inline">\(X\)</span> 的列向量化转置 <span class="math inline">\((\mathrm{vec} \boldsymbol X)^T\)</span> 求偏导，给出 <span class="math inline">\(pq \times mn\)</span> 维 Jacobian 矩阵</p>
<p><span class="math display">\[ \text D_\boldsymbol X \boldsymbol F(\boldsymbol X) \overset{\text{def}}{=} \frac{\partial{\mathrm{vec}(\boldsymbol F(\boldsymbol X))}}{\partial(\mathrm{vec} \boldsymbol X)^T} \in \Bbb R^{pq \times mn}\]</span></p>
<p>其具体的表达式为</p>
<p><span class="math display">\[ \text D_\boldsymbol X \boldsymbol F(\boldsymbol X)
= \begin{bmatrix}
\frac {\partial f_{11}}{\partial(\mathrm{vec} \boldsymbol X)^T} \\
\vdots \\
\frac {\partial f_{p1}}{\partial(\mathrm{vec} \boldsymbol X)^T} \\
\vdots \\
\frac {\partial f_{1q}}{\partial(\mathrm{vec} \boldsymbol X)^T} \\
\vdots \\
\frac {\partial f_{pq}}{\partial(\mathrm{vec} \boldsymbol X)^T} \\
\end{bmatrix}
= \begin{bmatrix}
\frac {\partial f_{11}}{\partial x_{11}} &amp; \cdots &amp; \frac {\partial f_{11}}{\partial x_{m1}} &amp; \cdots &amp; \frac {\partial f_{11}}{\partial x_{1n}} &amp; \cdots &amp; \frac {\partial f_{11}}{\partial x_{mn}} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\frac {\partial f_{p1}}{\partial x_{11}} &amp; \cdots &amp; \frac {\partial f_{p1}}{\partial x_{m1}} &amp; \cdots &amp; \frac {\partial f_{p1}}{\partial x_{1n}} &amp; \cdots &amp; \frac {\partial f_{p1}}{\partial x_{mn}} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\frac {\partial f_{1q}}{\partial x_{11}} &amp; \cdots &amp; \frac {\partial f_{1q}}{\partial x_{m1}} &amp; \cdots &amp; \frac {\partial f_{1q}}{\partial x_{1n}} &amp; \cdots &amp; \frac {\partial f_{1q}}{\partial x_{mn}} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\frac {\partial f_{pq}}{\partial x_{11}} &amp; \cdots &amp; \frac {\partial f_{pq}}{\partial x_{m1}} &amp; \cdots &amp; \frac {\partial f_{pq}}{\partial x_{1n}} &amp; \cdots &amp; \frac {\partial f_{pq}}{\partial x_{mn}} \\
\end{bmatrix}\]</span></p>
<p>梯度矩阵 <span class="math inline">\(\bigtriangledown_{\boldsymbol X} \boldsymbol F(\boldsymbol X)={(\text D_\boldsymbol X \boldsymbol F(\boldsymbol X))}^T\)</span>，也就是说矩阵函数的梯度矩阵是其 Jacobian 矩阵的转置。 简单来说，就是转换成<strong>一组标量函数对向量的求导</strong>。 <strong>但是，矩阵函数并不适用类似标量函数的链式法则！！！</strong> 这部分参考 <em>张贤达《矩阵分析与应用（第二部）》3.1 Jacobian矩阵与梯度矩阵</em></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://liulinyun.github.io/2018/03/29/拉格朗日乘子法和KKT条件/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="流粼">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiuLin的学习笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/29/拉格朗日乘子法和KKT条件/" itemprop="url">拉格朗日乘子法和KKT条件</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-29T15:48:41+08:00">
                2018-03-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h1 id="拉格朗日乘子法和kkt条件">拉格朗日乘子法和KKT条件</h1>
<h2 id="一关于拉格朗日乘子法">（一）关于拉格朗日乘子法</h2>
<p>首先来了解拉格朗日乘子法，那么为什么需要拉格朗日乘子法？记住，有拉格朗日乘子法的地方，必然是一个组合优化问题。那么带约束的优化问题很好说，就比如说下面这个： <span class="math display">\[\begin{aligned}
min &amp; \quad f = 2x_1^2+3x_2^2+7x_3^2 \\
s.t. &amp; \quad 2x_1+x_2 = 1 \\ 
     &amp; \quad 2x_2+3x_3 = 2
\end{aligned}\]</span></p>
<p>这是一个带等式约束的优化问题，有目标值，有约束条件。那么想想假设没有约束条件这个问题是怎么求解的呢？是不是直接 <span class="math inline">\(f\)</span> 对各个 <span class="math inline">\(x\)</span> 求导等于 0，解 <span class="math inline">\(x\)</span> 就可以了，可以看到没有约束的话，求导为0，那么各个 <span class="math inline">\(x\)</span> 均为 0 吧，这样 <span class="math inline">\(f=0\)</span> 了，最小。但是 <span class="math inline">\(x\)</span> 都为 0 不满足约束条件呀，那么问题就来了。这里在说一点的是，为什么上面说求导为0就可以呢？理论上多数问题是可以的，但是有的问题不可以。如果求导为 0 一定可以的话，那么 <span class="math inline">\(f\)</span> 一定是个凸优化问题，什么是凸的呢？如下图所示： <img src="/2018/03/29/拉格朗日乘子法和KKT条件/拉格朗日乘子法和KKT条件_图_1.png" alt="图一 什么是凸优化问题"> <strong>图一</strong> 什么是凸优化问题 凸的就是开口朝一个方向（向上或向下）。更准确的数学关系就是： <span class="math display">\[\quad \dfrac{f(x_1)+f(x_2)}{2} &gt;f(\dfrac{x_1+x_2}{2}) \quad \text{或者}\\ \dfrac{f(x_1)+f(x_2)}{2} &lt;f(\dfrac{x_1+x_2}{2})\]</span></p>
<p>注意的是这个条件是<strong>对函数的任意 <span class="math inline">\(x\)</span> 取值</strong>。如果满足第一个就是开口向上的凸，第二个是开口向下的凸。可以看到对于凸问题，你去求导的话，是不是只有一个极点，那么他就是最优点，很合理。类似的看看上图右边这个图，很明显这个条件对任意的 <span class="math inline">\(x\)</span> 取值不满足，有时满足第一个关系，有时满足第二个关系，对应上面的两处取法就是，所以这种问题就不行，再看看你去对它求导，会得到好几个极点。然而从图上可以看到，只有其中一个极点是最优解，其他的是局部最优解，那么当真实问题的时候你选择那个？说了半天要说啥呢，就是拉格朗日法是一定适合于凸问题的，不一定适合于其他问题，还好我们最终的问题是凸问题。 回头再来看看有约束的问题，既然有了约束不能直接求导，那么如果把约束去掉不就可以了吗？怎么去掉呢？这才需要拉格朗日方法。既然是等式约束，那么我们把这个约束乘一个系数加到目标函数中去，这样就相当于既考虑了原目标函数，也考虑了约束条件，比如上面那个函数，加进去就变为： <span class="math display">\[ min \quad f = 2x_1^2+3x_2^2+7x_3^2 +\alpha _1(2x_1+x_2- 1)+\alpha _2(2x_2+3x_3 - 2) \]</span> 这里可以看到与 <span class="math inline">\(\alpha_1,\alpha_2\)</span> 相乘的部分都为0，所以 <span class="math inline">\(\alpha_1,\alpha_2\)</span> 的取值为全体实数。现在这个优化目标函数就没有约束条件了吧，既然如此，求法就简单了，分别对 <span class="math inline">\(x\)</span> 求导等于0，如下： <span class="math display">\[ \dfrac{\partial f}{\partial x_1}=4x_1+2\alpha_1=0\Rightarrow x_1=-0.5\alpha_1 \\ \dfrac{\partial f}{\partial x_2}=6x_2+\alpha_1+2\alpha_2=0\Rightarrow x_2=-\dfrac{\alpha_1+2\alpha_2}{6} \\ \dfrac{\partial f}{\partial x_3}=14x_3+3\alpha_2=0\Rightarrow x_3=-\dfrac{3\alpha_2}{14} \]</span> 把它在带到约束条件中去，可以看到，2个变量两个等式，可以求解，最终可以得到 <span class="math inline">\(\alpha_1 = -0.39, \alpha_2 = -1.63\)</span>，这样再代回去求 <span class="math inline">\(x\)</span> 就可以了。那么一个带等式约束的优化问题就通过拉格朗日乘子法完美的解决了。那么更高一层的，带有不等式的约束问题怎么办？那么就需要用更一般化的拉格朗日乘子法即KKT条件来解决这种问题了。</p>
<h2 id="二关于kkt条件">（二）关于KKT条件</h2>
<p>继续讨论关于带等式以及不等式的约束条件的凸函数优化。任何原始问题约束条件无非最多3种，等式约束，大于号约束，小于号约束，而这三种最终通过将约束方程化简化为两类：约束方程等于0和约束方程小于0。再举个简单的方程为例，假设原始约束条件为下列所示： <span class="math display">\[ min \quad f = x_1^2-2x_1+1+x_2^2+4x_2+4 \\s.t. \quad x_1+10x_2 &gt; 10 \\ \quad \quad \quad 10 x_1-10x_2 &lt; 10 \]</span> 那么把约束条件变个样子： <span class="math display">\[ s.t. \quad 10-x_1-10x_2 &lt;0 \\ \quad \quad \quad 10x_1-x_2 - 10&lt;0 \]</span> 为什么都变成等号与小于号，方便后面的，反正式子的关系没有发生任何变化就行了。</p>
<p>现在将约束拿到目标函数中去就变成：</p>
<p><span class="math display">\[\begin{aligned}
L(x,\alpha) &amp; = f(x) + \alpha_1 g_1(x)+\alpha_2 g_2(x) \\ 
&amp; = x_1^2-2x_1+1+x_2^2+4x_2+4+ \alpha_1(10-x_1-10x_2 ) + \alpha_2(10x_1-x_2 - 10) 
\end{aligned}\]</span></p>
<p>那么KKT条件的定理是什么呢？就是如果一个优化问题</p>
<p><span class="math display">\[\begin{aligned}
\min_x &amp; \quad f(x) \\
s.t.   &amp; \quad g_i(x) \leq 0 \quad (i=1,2,\dots,m) \\ 
       &amp; \quad h_j(x) = 0 \quad (j=1,2,\dots,n)
\end{aligned}\]</span></p>
<p>引入相应的拉格朗日乘子变成:</p>
<p><span class="math display">\[L(x,\alpha,\beta) = f(x) + \sum \alpha_i g_i(x) + \sum \beta_j h_j(x)\]</span></p>
<p>其中<span class="math inline">\(g\)</span>是不等式约束，<span class="math inline">\(h\)</span>是等式约束（像上面那个只有不等式约束，也可能有等式约束）。那么KKT条件就是函数的最优值必定满足下面条件： (1) L对各个x求导为零； (2) $ h_j(x)=0 $; (3) $ _i g_i(x) = 0，_i 0 $ 也就是：</p>
<p><span class="math display">\[\begin{cases}
\dfrac {\partial L} {\partial x} = 0 \\
h_j(x) = 0 \\
\sum\alpha_i g_i(x) = 0，\alpha_i \geq 0
\end{cases}\]</span></p>
<p>第一二个式子易于理解。对于第三个式子，我们知道在约束条件变完后，所有的 <span class="math inline">\(g(x) \leq 0\)</span>，且 <span class="math inline">\(\alpha_i \geq 0\)</span>，然后对它们求和还要等于0，这就是说，要么某个不等式 <span class="math inline">\(\alpha_i = 0\)</span>，要么某个不等式 <span class="math inline">\(g_i(x) = 0\)</span>。因此我们可以把KKT条件写成如下易于使用的形式：</p>
<p><span class="math display">\[\begin{cases}
\dfrac {\partial L} {\partial x} = 0 \quad \text{拉格朗日乘子法} \\
h_j(x) = 0 \quad \text{等式约束}\\
g_i(x) \leq 0 \quad \text{不等式约束}\\
\alpha_i \geq 0 \\
\alpha_i g_i(x) = 0
\end{cases}\]</span></p>
<p>后面三个式子就是常用到的由不等式引入的KKT条件。那么为什么会是这样的呢？我们假设有一个目标函数以及它的约束条件，形象地画出来就是这个样子：</p>
<div class="figure">
<img src="/2018/03/29/拉格朗日乘子法和KKT条件/拉格朗日乘子法和KKT条件_图_2.png" alt="图二 什么是凸优化问题">
<p class="caption">图二 什么是凸优化问题</p>
</div>
<p>假设就这么几个不等式约束吧，最终约束是把自变量约束在一定范围，而函数是在这个范围内寻找最优解。函数开始也不知道该取哪一个值是吧，那就随便取一个，假设某一次取得自变量集合为 <span class="math inline">\(x_1^*\)</span>，发现一看，不满足约束，然后再换呀换，换到了 <span class="math inline">\(x_2^*\)</span>，发现可以了，但是这个时候函数值不是最优的，并且 <span class="math inline">\(x_2^*\)</span> 使得 <span class="math inline">\(g_1(x)\)</span> 与 <span class="math inline">\(g_2(x)\)</span> 等于0了，而 <span class="math inline">\(g_3(x)\)</span> 还是小于0。这个时候，我们发现在 <span class="math inline">\(x_2^*\)</span> 的基础上再寻找一组更优解要靠谁呢？当然是要靠约束条件 <span class="math inline">\(g_1(x)\)</span> 与 <span class="math inline">\(g_2(x)\)</span>，因为他们等于0了，很极限呀，一不小心，走错了就不满足它们两了，这个时候我们会选择 <span class="math inline">\(g_1(x)\)</span> 与 <span class="math inline">\(g_2(x)\)</span> 的梯度方向往下走，这样才能最大程度的摆脱 <span class="math inline">\(g_1(x)\)</span> 与 <span class="math inline">\(g_2(x)\)</span> 等于 0 的命运，使得他们满足小于 0 的约束条件对不对。至于这个时候需不需要管 <span class="math inline">\(g_3(x)\)</span> 呢？正常来说管不管都可以，如果管了，也取 <span class="math inline">\(g_3(x)\)</span> 在<span class="math inline">\(x_2^*\)</span> 处的梯度的话，因为 <span class="math inline">\(g_3(x)\)</span>已经满足了小于0的条件，这个时候在取在 <span class="math inline">\(x_2^*\)</span> 处的梯度，你能保证它是往好的变了还是往差的变了？答案是都有可能。运气好，往好的变了，可以更快得到结果，运气不好，往差的变了，反而适得其反。那么如果不管呢？因为 <span class="math inline">\(g_1(x)\)</span> 与 <span class="math inline">\(g_2(x)\)</span> 已经在边缘了，所以取它的梯度是一定会让目标函数变好的。综合来看，这个时候我们就不选 <span class="math inline">\(g_3\)</span>。那么再往下走，假设到了自变量优化到了 <span class="math inline">\(x_3^*\)</span>，这个时候发现 <span class="math inline">\(g_2(x)\)</span> 与 <span class="math inline">\(g_3(x)\)</span> 等于0，也就是走到边了，而 <span class="math inline">\(g_1(x)\)</span> 小于0，可变化的空间绰绰有余，那么这个时候举要取 <span class="math inline">\(g_2(x)\)</span> 与 <span class="math inline">\(g_3(x)\)</span> 的梯度方向作为变化的方向，而不用管 <span class="math inline">\(g_1(x)\)</span>。那么一直这样走呀走，最终找到最优解。可以看到的是，上述如果 <span class="math inline">\(g_1(x),g_2(x)=0\)</span> 的话，我们是需要优化它的，又因为他们本身的条件是小于0的，所以最终的公式推导上表明，是要乘以一个正系数 <span class="math inline">\(\alpha\)</span> 作为他们梯度增长的倍数，而那些不需要管的 <span class="math inline">\(g(x)\)</span> 为了统一表示，这个时候可以将这个系数设置为0，那么这一项在这一次的优化中就没有了。那么把这两种综合起来就可以表示为 <span class="math inline">\(\sum\alpha_ig_i(x)=0，\alpha_i\ge0\)</span>。也即是某次的 <span class="math inline">\(g(x)\)</span> 在为最优解起作用，那么它的系数值(可以)不为0。如果某次 <span class="math inline">\(g(x)\)</span> 没有为下一次的最优解<span class="math inline">\(x\)</span>的获得起到作用，那么它的系数就必须为0，这就是这个公式的含义。</p>
<p>用一句话来说就是：<strong>凸优化问题的最优解在约束条件构成的边界上</strong>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://liulinyun.github.io/2018/03/29/各种决策树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="流粼">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiuLin的学习笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/29/各种决策树/" itemprop="url">各种决策树</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-29T15:42:05+08:00">
                2018-03-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h1 id="各种决策树">各种决策树</h1>
<h2 id="基本决策树算法">1.基本决策树算法</h2>
<p>决策树算法的基本流程如下： ___________________ <strong>输入</strong>：训练集 <span class="math inline">\(D = \{(\boldsymbol x_1,y_1),(\boldsymbol x_2,y_2,\dots,(\boldsymbol x_m,y_m)\}\)</span>； <span class="math inline">\(\quad\quad\)</span> 属性集 <span class="math inline">\(A=\{a_1,a_2,\dots,a_d\}\)</span> <strong>过程</strong>：函数 TreeGenerate(<span class="math inline">\(D,A\)</span>)</p>
<ol style="list-style-type: decimal">
<li>生成节点 node；</li>
<li>if <span class="math inline">\(D\)</span> 中样本属于同一类别 <span class="math inline">\(C\)</span> then</li>
<li><span class="math inline">\(\quad\)</span> 将 node 标记为 <span class="math inline">\(C\)</span> 类节点；return</li>
<li>end if</li>
<li>if <span class="math inline">\(A = \varnothing\)</span> OR <span class="math inline">\(D\)</span> 中样本在 <span class="math inline">\(A\)</span> 上取值相同 then</li>
<li><span class="math inline">\(\quad\)</span> 将 node 标记为叶节点，其类别标记为 <span class="math inline">\(D\)</span> 中样本数最多的类；return</li>
<li>end if</li>
<li>从 <span class="math inline">\(A\)</span> 中选择最优划分属性 <span class="math inline">\(a_*\)</span> 作为 node 划分属性； //<strong>关键在于如何选择最优划分属性</strong></li>
<li>for <span class="math inline">\(a_*\)</span> 中的每一个值 <span class="math inline">\(a_*^v\)</span> do</li>
<li><span class="math inline">\(\quad\)</span> 为 node 生成一个分支；令 <span class="math inline">\(D_v\)</span> 表示 <span class="math inline">\(D\)</span> 中在 <span class="math inline">\(a_*\)</span> 上取值为 <span class="math inline">\(a_*^v\)</span> 的样本子集；</li>
<li><span class="math inline">\(\quad\)</span> if <span class="math inline">\(D_v\)</span> 为空 then</li>
<li><span class="math inline">\(\quad \quad\)</span> 将分支结点标记为叶节点，其类别标记为 <span class="math inline">\(D\)</span> 中样本最多的类；return</li>
<li><span class="math inline">\(\quad\)</span> else</li>
<li><span class="math inline">\(\quad \quad\)</span> 以 TreeGenerate(<span class="math inline">\(D_v,A \setminus \{a_*\}\)</span>) 为分支结点</li>
<li><span class="math inline">\(\quad\)</span> end if</li>
<li>end for</li>
</ol>
<p><strong>输出</strong>： 以 node 为根节点的一棵决策树 ___________________</p>
<p>从决策树算法的基本流程可以看出，有以下情形会导致递归返回： （1）当前结点包含的样本全属于同一类别，无需划分； （2）当前属性集为空，无法划分（这种情况出现在最长的划分路径末端，所有属性都被用过为划分属性，但是样本集合中还剩下异类样本）； （3）所有样本在所有属性值上取值相同，无法划分； （4）当前结点包含的样本集合为空，不能划分。</p>
<p>举个栗子： 来自西瓜数据集2</p>
<table>
<thead>
<tr class="header">
<th>编号</th>
<th align="right">色泽</th>
<th align="right">根蒂</th>
<th align="right">敲声</th>
<th align="right">纹理</th>
<th align="right">脐部</th>
<th align="right">触感</th>
<th>好瓜</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td align="right">青绿</td>
<td align="right">蜷缩</td>
<td align="right">浊响</td>
<td align="right">清晰</td>
<td align="right">凹陷</td>
<td align="right">硬滑</td>
<td>是</td>
</tr>
<tr class="even">
<td>2</td>
<td align="right">乌黑</td>
<td align="right">蜷缩</td>
<td align="right">沉闷</td>
<td align="right">清晰</td>
<td align="right">凹陷</td>
<td align="right">硬滑</td>
<td>是</td>
</tr>
<tr class="odd">
<td>3</td>
<td align="right">乌黑</td>
<td align="right">蜷缩</td>
<td align="right">浊响</td>
<td align="right">清晰</td>
<td align="right">凹陷</td>
<td align="right">硬滑</td>
<td>是</td>
</tr>
<tr class="even">
<td>4</td>
<td align="right">青绿</td>
<td align="right">蜷缩</td>
<td align="right">沉闷</td>
<td align="right">清晰</td>
<td align="right">凹陷</td>
<td align="right">硬滑</td>
<td>是</td>
</tr>
<tr class="odd">
<td>5</td>
<td align="right">浅白</td>
<td align="right">蜷缩</td>
<td align="right">浊响</td>
<td align="right">清晰</td>
<td align="right">凹陷</td>
<td align="right">硬滑</td>
<td>是</td>
</tr>
<tr class="even">
<td>6</td>
<td align="right">青绿</td>
<td align="right">稍蜷</td>
<td align="right">浊响</td>
<td align="right">清晰</td>
<td align="right">稍凹</td>
<td align="right">软粘</td>
<td>是</td>
</tr>
<tr class="odd">
<td>7</td>
<td align="right">乌黑</td>
<td align="right">稍蜷</td>
<td align="right">浊响</td>
<td align="right">稍糊</td>
<td align="right">稍凹</td>
<td align="right">软粘</td>
<td>是</td>
</tr>
<tr class="even">
<td>8</td>
<td align="right">乌黑</td>
<td align="right">稍蜷</td>
<td align="right">浊响</td>
<td align="right">清晰</td>
<td align="right">稍凹</td>
<td align="right">硬滑</td>
<td>是</td>
</tr>
<tr class="odd">
<td>9</td>
<td align="right">乌黑</td>
<td align="right">稍蜷</td>
<td align="right">沉闷</td>
<td align="right">稍糊</td>
<td align="right">稍凹</td>
<td align="right">硬滑</td>
<td>否</td>
</tr>
<tr class="even">
<td>10</td>
<td align="right">青绿</td>
<td align="right">硬挺</td>
<td align="right">清脆</td>
<td align="right">清晰</td>
<td align="right">平坦</td>
<td align="right">软粘</td>
<td>否</td>
</tr>
<tr class="odd">
<td>11</td>
<td align="right">浅白</td>
<td align="right">硬挺</td>
<td align="right">清脆</td>
<td align="right">模糊</td>
<td align="right">平坦</td>
<td align="right">硬滑</td>
<td>否</td>
</tr>
<tr class="even">
<td>12</td>
<td align="right">浅白</td>
<td align="right">蜷缩</td>
<td align="right">浊响</td>
<td align="right">模糊</td>
<td align="right">平坦</td>
<td align="right">软粘</td>
<td>否</td>
</tr>
<tr class="odd">
<td>13</td>
<td align="right">青绿</td>
<td align="right">稍蜷</td>
<td align="right">浊响</td>
<td align="right">稍糊</td>
<td align="right">凹陷</td>
<td align="right">硬滑</td>
<td>否</td>
</tr>
<tr class="even">
<td>14</td>
<td align="right">浅白</td>
<td align="right">稍蜷</td>
<td align="right">沉闷</td>
<td align="right">稍糊</td>
<td align="right">凹陷</td>
<td align="right">硬滑</td>
<td>否</td>
</tr>
<tr class="odd">
<td>15</td>
<td align="right">乌黑</td>
<td align="right">稍蜷</td>
<td align="right">浊响</td>
<td align="right">清晰</td>
<td align="right">稍凹</td>
<td align="right">软粘</td>
<td>否</td>
</tr>
<tr class="even">
<td>16</td>
<td align="right">浅白</td>
<td align="right">蜷缩</td>
<td align="right">浊响</td>
<td align="right">模糊</td>
<td align="right">平坦</td>
<td align="right">硬滑</td>
<td>否</td>
</tr>
<tr class="odd">
<td>17</td>
<td align="right">青绿</td>
<td align="right">蜷缩</td>
<td align="right">沉闷</td>
<td align="right">稍糊</td>
<td align="right">稍凹</td>
<td align="right">硬滑</td>
<td>否</td>
</tr>
</tbody>
</table>
<p>假设我们以<strong>信息熵</strong>（后面会讲）作为最优划分属性的选择标准，决策树的执行流程如下图所示 <img src="/2018/03/29/各种决策树/决策树执行过程.jpg" alt="决策树执行过程"></p>
<h2 id="划分选择">2.划分选择</h2>
<h3 id="信息增益">2.1 信息增益</h3>
<p>“信息熵”是度量样本集合纯度最常用的一种指标。假定当前样本集合 <span class="math inline">\(D\)</span> 中第 <span class="math inline">\(k\)</span> 类样本所占的比例为 <span class="math inline">\(p_k \quad (k=1,2,\dots,|\mathcal Y|)\)</span>，则 <span class="math inline">\(D\)</span> 的信息熵定义为 <span class="math display">\[ \text{Ent}(D) = - \sum_{k=1}^{|\mathcal Y|} {p_k \log_2 p_k}\]</span> 信息熵越小，则 <span class="math inline">\(D\)</span> 的纯度越高。</p>
<p>假定离散属性 <span class="math inline">\(a\)</span> 有 <span class="math inline">\(V\)</span> 个可能的取值 <span class="math inline">\(\{a^1,a^2,\dots,a^V\}\)</span>，若使用 <span class="math inline">\(a\)</span> 来对样本集 <span class="math inline">\(D\)</span> 进行划分，则会产生 <span class="math inline">\(V\)</span> 个分支节点，其中第 <span class="math inline">\(v\)</span> 个结点包含了 <span class="math inline">\(D\)</span> 中所有在属性 <span class="math inline">\(a\)</span> 上取值为 <span class="math inline">\(a^v\)</span> 的样本，记为 <span class="math inline">\(D^v\)</span>。我们可根据上式计算出 <span class="math inline">\(D^v\)</span> 的信息熵，再考虑到不同分支结点所包含的样本数不同，给分支结点赋予权重 <span class="math inline">\(|D^v|/|D|\)</span>，即样本数最多的分支结点的影响最大，于是可以计算出用属性 <span class="math inline">\(a\)</span> 对样本集 <span class="math inline">\(D\)</span> 进行划分所获得的“信息增益” <span class="math display">\[ \text{Gain} (D,a) = \text{Ent}(D) - \sum_{v=1}^V {\frac {|D^v|}{|D|} \text{Ent}(D^v)} \]</span> 一般而言，信息增益越大，意味着使用属性 <span class="math inline">\(a\)</span> 来划分所获得的“纯度提升”越大。因此可以选择属性 <span class="math inline">\(a_* = \arg \displaystyle \max_{a \in A} \text{Gain}(D,a)\)</span> 来划分属性。 使用信息增益作为准则来选择划分属性的决策树就是著名的 <strong>ID3决策树</strong>。</p>
<h3 id="信息增益率">2.2 信息增益率</h3>
<p><strong>C4.5决策树</strong>不直接使用信息增益，而是使用“信息增益率”来选择最优划分属性。增益率定义为 <span class="math display">\[ \text{Gain_ratio}(D,a) = \frac {\text{Gain} (D,a)} {\text{IV}(a)}\]</span> 其中 <span class="math display">\[ \text{IV}(a) = - \sum_{v=1}^V{\frac {|D^v|}{|D|} \log_2 \frac {|D^v|}{|D|}} \]</span> 称为属性 <span class="math inline">\(a\)</span> 的“固有值”。属性 <span class="math inline">\(a\)</span> 的可能取值数目越多（<span class="math inline">\(V\)</span> 越大），则 <span class="math inline">\(\text{IV}(a)\)</span> 的值通常会越大。因此，直接按增益率来选择可能对取值数目较少的属性有所偏好，因此，C4.5 不直接选择增益率最大的候选划分属性，而是使用一个启发式：<strong>先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的那个，作为最后选择的最优划分属性。</strong></p>
<h3 id="基尼指数">2.3 基尼指数</h3>
<p><strong>CART决策树</strong> 使用“基尼指数”来选择划分属性。数据集 <span class="math inline">\(D\)</span> 的纯度如果用基尼值来衡量则为 <span class="math display">\[ \begin{aligned}
\text{Gini}(D) &amp; = \sum_{k=1}^{|\mathcal Y|} \sum_{k&#39; \neq k} p_k p_{k&#39;} \\
&amp; =1-\sum_{k=1}^{|\mathcal Y|}p_k^2
\end{aligned}\]</span> 直观来说，<span class="math inline">\(D\)</span> 的基尼值反映了从数据集 <span class="math inline">\(D\)</span> 中随机抽取两个样本，其类别标记不一样的概率。因此，基尼值越小，则数据集的纯度越高。 类似信息增益的定义，属性 <span class="math inline">\(a\)</span> 的基尼指数定义为 <span class="math display">\[ \text{Gini_index}(D,a) = \sum_{v=1}^V {\frac {|D^v|}{|D|} \text{Gini}(D^v)} \]</span> 于是，我们在候选属性集 <span class="math inline">\(A\)</span> 中，选择基尼指数最小的属性作为最有划分属性，即 <span class="math inline">\(a_* = \arg \displaystyle \min_{a \in A} \text{Gini_index}(D,a)\)</span>。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://liulinyun.github.io/2018/03/29/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="流粼">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LiuLin的学习笔记">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/29/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-29T13:40:00+08:00">
                2018-03-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="quick-start">Quick Start</h2>
<h3 id="create-a-new-post">Create a new post</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="run-server">Run server</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="generate-static-files">Generate static files</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="deploy-to-remote-sites">Deploy to remote sites</h3>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
<p><span class="math display">\[ E=mc^2 \tag{1} \]</span></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">流粼</p>
              <p class="site-description motion-element" itemprop="description">学习笔记、读书笔记、生活感悟</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Liulinyun" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:liulinyun-oo@foxmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">流粼</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  





  
  







  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
